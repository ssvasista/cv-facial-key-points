{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main parts to this project:\n",
    "\n",
    "**Part 1** : Investigating OpenCV, pre-processing, and face detection\n",
    "\n",
    "**Part 2** : Training a Convolutional Neural Network (CNN) to detect facial keypoints\n",
    "\n",
    "**Part 3** : Putting parts 1 and 2 together to identify facial keypoints on any image!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1** : Investigating OpenCV, pre-processing, and face detection\n",
    "\n",
    "* [Step 0](#step0): Detect Faces Using a Haar Cascade Classifier\n",
    "* [Step 1](#step1): Add Eye Detection\n",
    "* [Step 2](#step2): De-noise an Image for Better Face Detection\n",
    "* [Step 3](#step3): Blur an Image and Perform Edge Detection\n",
    "* [Step 4](#step4): Automatically Hide the Identity of an Individual\n",
    "\n",
    "**Part 2** : Training a Convolutional Neural Network (CNN) to detect facial keypoints\n",
    "\n",
    "* [Step 5](#step5): Create a CNN to Recognize Facial Keypoints\n",
    "* [Step 6](#step6): Compile and Train the Model\n",
    "* [Step 7](#step7): Visualize the Loss and Answer Questions\n",
    "\n",
    "**Part 3** : Putting parts 1 and 2 together to identify facial keypoints on any image!\n",
    "\n",
    "* [Step 8](#step7): Build a Robust Facial Keypoints Detector (Complete the CV Pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Detect Faces Using a Haar Cascade Classifier\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `detector_architectures` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1006d8ded8c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m                     \u001b[1;31m# OpenCV library for computer vision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Import required libraries for this section\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2                     # OpenCV library for computer vision\n",
    "from PIL import Image\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in color image for face detection\n",
    "image = cv2.imread('images/test_image_1.jpg')\n",
    "\n",
    "# Convert the image to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot our image using subplots to specify a size and title\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RGB  image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the faces in image\n",
    "faces = face_cascade.detectMultiScale(gray, 4, 6)\n",
    "\n",
    "# Print the number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# Make a copy of the orginal image to draw face detections on\n",
    "image_with_detections = np.copy(image)\n",
    "\n",
    "# Get the bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # Add a red bounding box to the detections image\n",
    "    cv2.rectangle(image_with_detections, (x,y), (x+w,y+h), (255,0,0), 3)\n",
    "    \n",
    "# Display the image with the detections\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Image with Face Detections')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "\n",
    "## Step 1: Add Eye Detections\n",
    "\n",
    "There are other pre-trained detectors available that use a Haar Cascade Classifier - including full human body detectors, license plate detectors, and more.  [A full list of the pre-trained architectures can be found here](https://github.com/opencv/opencv/tree/master/data/haarcascades). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your eye detector, we'll first read in a new test image with just a single face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in color image for face detection\n",
    "image = cv2.imread('images/fawzia.jpg')\n",
    "\n",
    "# Convert the image to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot the RGB image\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RGB  image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the faces in image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "\n",
    "# Print the number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# Make a copy of the orginal image to draw face detections on\n",
    "image_with_detections = np.copy(image)\n",
    "\n",
    "# Get the bounding box for each detected face\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Add a red bounding box to the detections image\n",
    "    cv2.rectangle(image_with_detections, (x_face,y_face), (x_face+w_face,y_face+h_face), (255,0,0), 3)\n",
    "\n",
    "# Display the image with the detections\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Image with Face Detection')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an eye detector to the current face detection setup.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5393858226b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make a copy of the original image to plot rectangle detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage_with_detections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Loop over the detections and draw their corresponding face detection boxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "# Make a copy of the original image to plot rectangle detections\n",
    "image_with_detections = np.copy(image)   \n",
    "\n",
    "# Loop over the detections and draw their corresponding face detection boxes\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(image_with_detections, (x,y), (x+w,y+h),(255,0,0), 3)  \n",
    "    \n",
    "# Do not change the code above this comment!\n",
    "    \n",
    "## TODO: Add eye detection, using haarcascade_eye.xml, to the current face detector algorithm\n",
    "## TODO: Loop over the eye detections and draw their corresponding boxes in green on image_with_detections\n",
    "\n",
    "# Extract the pre-trained eye detector from an xml file\n",
    "eye_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_eye.xml')\n",
    "\n",
    "# Detect the eyes within each face rectangle\n",
    "# Note: had to bring the scaleFactor param down a bit to make this work\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Limit eye detection to the face rectangle to speed things up\n",
    "    eyes = eye_cascade.detectMultiScale(image_with_detections[y_face:y_face+h_face, x_face:x_face+w_face], 1.10, 6)\n",
    "    for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "        # Add a green bounding box to the detections image for each eye\n",
    "        # Note that eye coordinates are are relative to the face rectangle now!\n",
    "        cv2.rectangle(image_with_detections, (x_face+x_eye,y_face+y_eye), (x_face+x_eye+w_eye,y_face+y_eye+h_eye), (0,255,0), 3)\n",
    "\n",
    "# Plot the image with both faces and eyes detected\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Image with Face and Eye Detection')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a5e45b2d7137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Make sure to draw out all faces/eyes found in each frame on the shown video feed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "### Add face and eye detection to this laptop camera function \n",
    "# Make sure to draw out all faces/eyes found in each frame on the shown video feed\n",
    "\n",
    "import cv2\n",
    "import time \n",
    "\n",
    "# wrapper function for face/eye detection with your laptop camera\n",
    "def laptop_camera_go():\n",
    "    # Create instance of video capturer\n",
    "    cv2.namedWindow(\"Face+Eye Detector [Press ESC to exit]\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    # Try to get the first frame\n",
    "    if vc.isOpened(): \n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    \n",
    "    # Extract the pre-trained face and eye detectors from xml files\n",
    "    face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_eye.xml')\n",
    "\n",
    "    # Keep the video stream open\n",
    "    while rval:\n",
    "        \n",
    "        # Make a copy of the original color image to plot rectangle detections\n",
    "        image_with_detections = np.copy(frame)   \n",
    "\n",
    "        # Convert the RGB video frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Detect the face(s) in the video frame\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, 1.25, 6)\n",
    "        \n",
    "        # Detect the eyes within each face rectangle\n",
    "        # Note: had to bring the scaleFactor param down a bit to make this work\n",
    "        for (x_face,y_face,w_face,h_face) in faces:\n",
    "            # Display each face rectangle in the color image\n",
    "            cv2.rectangle(image_with_detections, (x_face,y_face), (x_face+w_face,y_face+h_face), (0,0,255), 3)  \n",
    "            # Limit eye detection to the face rectangle to speed things up\n",
    "            eyes = eye_cascade.detectMultiScale(gray_frame[y_face:y_face+h_face, x_face:x_face+w_face], 1.10, 6)\n",
    "            for (x_eye,y_eye,w_eye,h_eye) in eyes:\n",
    "                # Add a green bounding box to the detections image for each eye\n",
    "                # Note that eye coordinates are are relative to the face rectangle now!\n",
    "                cv2.rectangle(image_with_detections, (x_face+x_eye,y_face+y_eye), (x_face+x_eye+w_eye,y_face+y_eye+h_eye), (0,255,0), 3)\n",
    "\n",
    "        # Plot the image from camera with all the face and eye detections marked\n",
    "        cv2.imshow(\"Face+Eye Detector [Press ESC to exit]\", image_with_detections)\n",
    "        \n",
    "        # Exit functionality - press any key to exit laptop video\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # Exit by pressing ESC key\n",
    "            # Destroy windows \n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # Make sure window closes on OSx\n",
    "            for i in range (1,5):\n",
    "                cv2.waitKey(1)\n",
    "            return\n",
    "        \n",
    "        # Read next frame\n",
    "        time.sleep(0.05)             # control framerate for computation - default 20 frames per sec\n",
    "        rval, frame = vc.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laptop_camera_go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0144377e03a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Call the laptop camera face/eye detector function above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'laptop_camera_go' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the laptop camera face/eye detector function above\n",
    "laptop_camera_go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "\n",
    "## Step 2: De-noise an Image for Better Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image quality is an important aspect of any computer vision task. Typically, when creating a set of images to train a deep learning network, significant care is taken to ensure that training images are free of visual noise or artifacts that hinder object detection.  While computer vision algorithms - like a face detector - are typically trained on 'nice' data such as this, new test data doesn't always look so nice!\n",
    "\n",
    "When applying a trained computer vision algorithm to a new piece of test data one often cleans it up first before feeding it in.  This sort of cleaning - referred to as *pre-processing* - can include a number of cleaning phases like blurring, de-noising, color transformations, etc., and many of these tasks can be accomplished using OpenCV.\n",
    "\n",
    "In this short subsection we explore OpenCV's noise-removal functionality to see how we can clean up a noisy image, which we then feed into our trained face detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a noisy image to work with\n",
    "\n",
    "In the next cell, we create an artificial noisy version of the previous multi-face image.  This is a little exaggerated - we don't typically get images that are this noisy - but [image noise](https://digital-photography-school.com/how-to-avoid-and-reduce-noise-in-your-images/), or 'grainy-ness' in a digitial image - is a fairly common phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4c8ff3eccb8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in the multi-face test image again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/test_image_1.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert the image copy to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in the multi-face test image again\n",
    "image = cv2.imread('images/test_image_1.jpg')\n",
    "\n",
    "# Convert the image copy to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Make an array copy of this image\n",
    "image_with_noise = np.asarray(image)\n",
    "\n",
    "# Create noise - here we add noise sampled randomly from a Gaussian distribution: a common model for noise\n",
    "noise_level = 40\n",
    "noise = np.random.randn(image.shape[0],image.shape[1],image.shape[2])*noise_level\n",
    "\n",
    "# Add this noise to the array image copy\n",
    "image_with_noise = image_with_noise + noise\n",
    "\n",
    "# Convert back to uint8 format\n",
    "image_with_noise = np.asarray([np.uint8(np.clip(i,0,255)) for i in image_with_noise])\n",
    "\n",
    "# Plot our noisy image!\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Noisy Image')\n",
    "ax1.imshow(image_with_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-efd93a2ebcb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert the RGB  image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgray_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_with_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Extract the pre-trained face detector from an xml file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'detector_architectures/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the RGB  image to grayscale\n",
    "gray_noise = cv2.cvtColor(image_with_noise, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the faces in image\n",
    "faces = face_cascade.detectMultiScale(gray_noise, 4, 6)\n",
    "\n",
    "# Print the number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# Make a copy of the orginal image to draw face detections on\n",
    "image_with_detections = np.copy(image_with_noise)\n",
    "\n",
    "# Get the bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # Add a red bounding box to the detections image\n",
    "    cv2.rectangle(image_with_detections, (x,y), (x+w,y+h), (255,0,0), 3)\n",
    "    \n",
    "\n",
    "# Display the image with the detections\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Noisy Image with Face Detections')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this added noise we now miss one of the faces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  De-noise this image for better face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b16124fa1f1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# templateWindowSize : Size in pixels of the template patch that is used to compute weights. Should be odd. (recommended 7)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# searchWindowSize : Size in pixels of the window that is used to compute weighted average for given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater denoising time. (recommended 21)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdenoised_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfastNlMeansDenoisingColored\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_with_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Plot our denoised image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: Use OpenCV's built in color image de-noising function to clean up our noisy image!\n",
    "\n",
    "# cv2.fastNlMeansDenoisingColored(src[, dst[, h[, hColor[, templateWindowSize[, searchWindowSize]]]]]) -> dst\n",
    "# Common arguments are:\n",
    "# src : Input 8-bit 3-channel image\n",
    "# dst : Output image with the same size and type as src\n",
    "# h : Parameter regulating filter strength for luminance component. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise. (20 is ok)\n",
    "# hColor :  The same as h but for color components. For most images value equals 10 will be enought to remove colored noise and do not distort colors. (20 is ok)\n",
    "# templateWindowSize : Size in pixels of the template patch that is used to compute weights. Should be odd. (recommended 7)\n",
    "# searchWindowSize : Size in pixels of the window that is used to compute weighted average for given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater denoising time. (recommended 21)\n",
    "denoised_image = cv2.fastNlMeansDenoisingColored(image_with_noise, None, 20, 20, 7, 21)\n",
    "\n",
    "# Plot our denoised image\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Denoised Image')\n",
    "ax1.imshow(denoised_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bc156855333f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Extract the pre-trained face detector from an xml file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'detector_architectures/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Make a copy of the original denoised image to plot rectangle detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: Run the face detector on the de-noised image to improve your detections and display the result\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Make a copy of the original denoised image to plot rectangle detections\n",
    "image_with_detections = np.copy(denoised_image)   \n",
    "\n",
    "# Convert the denoised image to grayscale\n",
    "gray_denoised_image = cv2.cvtColor(denoised_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Detect the face(s) in the greyscale denoised image\n",
    "faces = face_cascade.detectMultiScale(gray_denoised_image, 1.25, 6)\n",
    "\n",
    "# Display each face rectangle in the denoised image\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    cv2.rectangle(image_with_detections, (x_face,y_face), (x_face+w_face,y_face+h_face), (255,0,0), 3)  \n",
    "\n",
    "# Plot the denoised image with all the face detections marked\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Denoised Image with Face Detections')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "\n",
    "## Step 3: Blur an Image and Perform Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny edge detection\n",
    "\n",
    "In the cell below we load in a test image, then apply *Canny edge detection* on it.  The original image is shown on the left panel of the figure, while the edge-detected version of the image is shown on the right.  Notice how the result looks very busy - there are too many little details preserved in the image before it is sent to the edge detector.  When applied in computer vision applications, edge detection should preserve *global* structure; doing away with local structures that don't help describe what objects are in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b2565e7ab7c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/fawzia.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in the image\n",
    "image = cv2.imread('images/fawzia.jpg')\n",
    "\n",
    "# Convert to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  \n",
    "\n",
    "# Perform Canny edge detection\n",
    "edges = cv2.Canny(gray,100,200)\n",
    "\n",
    "# Dilate the image to amplify edges\n",
    "edges = cv2.dilate(edges, None)\n",
    "\n",
    "# Plot the RGB and edge-detected image\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(image)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax2.set_title('Canny Edges')\n",
    "ax2.imshow(edges, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur the image *then* perform edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f93b46405f13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/fawzia.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Convert to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "### TODO: Blur the test imageusing OpenCV's filter2d functionality, \n",
    "# Use an averaging kernel, and a kernel width equal to 4\n",
    "\n",
    "# Load in the image\n",
    "image = cv2.imread('images/fawzia.jpg')\n",
    "\n",
    "# Convert to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  \n",
    "\n",
    "# Blur the greyscale image using OpenCV's filter2d() and a 4x4 averaging filter\n",
    "blurred_gray = cv2.filter2D(gray, -1, np.ones((4,4), np.float32)/16)\n",
    "\n",
    "## TODO: Then perform Canny edge detection and display the output\n",
    "\n",
    "# Perform Canny edge detection\n",
    "edges = cv2.Canny(blurred_gray,100,200)\n",
    "\n",
    "# Dilate the image to amplify edges\n",
    "edges = cv2.dilate(edges, None)\n",
    "\n",
    "# Plot the RGB and edge-detected image\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Blurred Image')\n",
    "ax1.imshow(blurred_gray, cmap='gray')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax2.set_title('Canny Edges from Blurred Image')\n",
    "ax2.imshow(edges, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "\n",
    "## Step 4: Automatically Hide the Identity of an Individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in an image to perform identity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c1cd3639422c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/gus.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert the image to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in the image\n",
    "image = cv2.imread('images/gus.jpg')\n",
    "\n",
    "# Convert the image to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blurring to hide the identity of an individual in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9a007082be7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Extract the pre-trained face detector from an xml file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'detector_architectures/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimage_with_blurred_faces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: Implement face detection\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "image_with_blurred_faces = np.copy(image)\n",
    "\n",
    "# Denoise the original image to make it easier on the face detector\n",
    "denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 20, 20, 7, 21)\n",
    "\n",
    "# Convert the denoised image to grayscale\n",
    "gray = cv2.cvtColor(denoised_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Detect the face(s) in the greyscale denoised image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "\n",
    "## TODO: Blur the bounding box around each detected face using an averaging filter and display the result\n",
    "\n",
    "# Create a strong averaging filter kernel\n",
    "blurring_kernel = np.ones((92,92), np.float32)/(92*92)\n",
    "\n",
    "# Blur the face(s) using OpenCV's filter2d() and a 4x4 averaging filter\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    image_with_blurred_faces[y_face:y_face+h_face, x_face:x_face+w_face] = cv2.filter2D(image[y_face:y_face+h_face, x_face:x_face+w_face], -1, blurring_kernel)\n",
    "\n",
    "# Plot the image with the face(s) blurred\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Image with Blurred Face(s)')\n",
    "ax1.imshow(image_with_blurred_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-35f9a3b89da0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Insert face detection and blurring code into the wrapper below to create an identity protector on your laptop!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "### Insert face detection and blurring code into the wrapper below to create an identity protector on your laptop!\n",
    "import cv2\n",
    "import time \n",
    "\n",
    "def laptop_camera_go():\n",
    "    # Create instance of video capturer\n",
    "    cv2.namedWindow(\"Identity Protector [Press ESC to exit]\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    # Try to get the first frame\n",
    "    if vc.isOpened(): \n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    \n",
    "    # Extract the pre-trained face detector from an xml file\n",
    "    face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Create a strong averaging filter kernel\n",
    "    blurring_kernel = np.ones((92,92), np.float32)/(92*92)\n",
    "\n",
    "    # Keep video stream open\n",
    "    while rval:\n",
    "        # Convert the RGB video frame to grayscale and detect the face(s) in it\n",
    "        faces = face_cascade.detectMultiScale(cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY), 1.25, 6)\n",
    "        \n",
    "        # Blur the detected faces\n",
    "        for (x_face,y_face,w_face,h_face) in faces:\n",
    "            frame[y_face:y_face+h_face, x_face:x_face+w_face] = cv2.filter2D(frame[y_face:y_face+h_face, x_face:x_face+w_face], -1, blurring_kernel)\n",
    "\n",
    "        # Plot image from camera with identity protected\n",
    "        cv2.imshow(\"Identity Protector [Press ESC to exit]\", frame)\n",
    "        \n",
    "        # Exit functionality\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # Exit by pressing ESC key\n",
    "            # Destroy windows\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            for i in range (1,5):\n",
    "                cv2.waitKey(1)\n",
    "            return\n",
    "        \n",
    "        # Read next frame\n",
    "        time.sleep(0.05)             # control framerate for computation - default 20 frames per sec\n",
    "        rval, frame = vc.read()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laptop_camera_go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6687f8301aa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run laptop identity hider\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'laptop_camera_go' is not defined"
     ]
    }
   ],
   "source": [
    "# Run laptop identity hider\n",
    "laptop_camera_go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "\n",
    "## Step 5: Create a CNN to Recognize Facial Keypoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a facial keypoint detector\n",
    "\n",
    "But first things first: how can we make a facial keypoint detector?  Well, at a high level, notice that facial keypoint detection is a *regression problem*.  A single face corresponds to a set of 15 facial keypoints (a set of 15 corresponding $(x, y)$ coordinates, i.e., an output point).  Because our input data are images, we can employ a *convolutional neural network* to recognize patterns in our images and learn how to identify these keypoint given sets of labeled data.\n",
    "\n",
    "In order to train a regressor, we need a training set - a set of facial image / facial keypoint pairs to train on.  For this we will be using [this dataset from Kaggle](https://www.kaggle.com/c/facial-keypoints-detection/data). We've already downloaded this data and placed it in the `data` directory. Make sure that you have both the *training* and *test* data files.  The training dataset contains several thousand $96 \\times 96$ grayscale images of cropped human faces, along with each face's 15 corresponding facial keypoints (also called landmarks) that have been placed by hand, and recorded in $(x, y)$ coordinates.  This wonderful resource also has a substantial testing set, which we will use in tinkering with our convolutional network.\n",
    "\n",
    "To load in this data, run the Python cell below - notice we will load in both the training and testing sets.\n",
    "\n",
    "The `load_data` function is in the included `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2ad797e13f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_train.shape == {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\cv-facialkeypoints\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "# Load training set\n",
    "X_train, y_train = load_data()\n",
    "print(\"X_train.shape == {}\".format(X_train.shape))\n",
    "print(\"y_train.shape == {}; y_train.min == {:.3f}; y_train.max == {:.3f}\".format(\n",
    "    y_train.shape, y_train.min(), y_train.max()))\n",
    "\n",
    "# Load testing set\n",
    "X_test, _ = load_data(test=True)\n",
    "print(\"X_test.shape == {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_data` function in `utils.py` originates from this excellent [blog post](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/), which you are *strongly* encouraged to read.  Please take the time now to review this function.  Note how the output values - that is, the coordinates of each set of facial landmarks - have been normalized to take on values in the range $[-1, 1]$, while the pixel values of each input point (a facial image) have been normalized to the range $[0,1]$.  \n",
    "\n",
    "Note: the original Kaggle dataset contains some images with several missing keypoints.  For simplicity, the `load_data` function removes those images with missing labels from the dataset.  As an __*optional*__ extension, you are welcome to amend the `load_data` function to include the incomplete data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Training Data\n",
    "\n",
    "Execute the code cell below to visualize a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c839bdbe6630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mplot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_data' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1edfa72a198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(X_train[i], y_train[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each training image, there are two landmarks per eyebrow (**four** total), three per eye (**six** total), **four** for the mouth, and **one** for the tip of the nose.  \n",
    "\n",
    "Review the `plot_data` function in `utils.py` to understand how the 30-dimensional training labels in `y_train` are mapped to facial locations, as this function will prove useful for your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-022e5daf3707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import deep learning resources from Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import deep learning resources from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "## TODO: Specify a CNN architecture\n",
    "# Your model should accept 96x96 pixel graysale images in\n",
    "# It should have a fully-connected output layer with 30 values (2 for each facial keypoint)\n",
    "model = Sequential()\n",
    "\n",
    "# Hyperparams tuned before settling on this final architecture:\n",
    "# kernel_size [2,3,4] -> 3 generalizes better than 2, 4 also adds improvement but requires more training epochs to perform well\n",
    "# number of convolution+maxpooling layers [3,4] -> 4 generalizes better, so we 're capturing meaningful higher level image features\n",
    "# tried batchnorm() after convolutional layers with no success (expected?)\n",
    "model.add(Convolution2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(96, 96, 1)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Convolution2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Convolution2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Convolution2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "# Hyperparams tuned before settling on this final architecture:\n",
    "# Add an extra dense layer so the model can come up with more sophisticated boundary decision regions\n",
    "# Dropout rates [0.25, 0.3, 0.4, 0.5] -> got best generalization results at 0.5\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(30))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "\n",
    "## Step 6: Compile and Train the Model\n",
    "\n",
    "After specifying your architecture, you'll need to compile and train the model to detect facial keypoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3e7f25408344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdagrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdadelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdamax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNadam\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m## Hypertune the optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pickle\n",
    "\n",
    "## Hypertune the optimizer\n",
    "optimizers = [SGD(), RMSprop(), Adagrad(), Adadelta(), Adam(), Adamax(), Nadam()]\n",
    "names = [\"CNN_2472350_SGD\", \"CNN_2472350_RMSprop\", \"CNN_2472350_Adagrad\", \"CNN_2472350_Adadelta\", \"CNN_2472350_Adam\", \"CNN_2472350_Adamax\", \"CNN_2472350_Nadam\"]\n",
    "histories = {}\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "for optimizer,name in zip(optimizers, names):\n",
    "    print(\"Evaluating \" + name)\n",
    "    model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mse'])\n",
    "    checkpointer = ModelCheckpoint(filepath = \"saved_models/weights_best_val_MSE_\" + name + \".hdf5\", verbose = 1, save_best_only = True)\n",
    "    histories[name] = model.fit(X_train, y_train, validation_split = 0.2, epochs = epochs, batch_size = batch_size, callbacks = [checkpointer], verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3927b392987e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Lifted from http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Plot the validation MSEs of all the models trained above with different optimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validation MSEs with Different Optimizers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'names' is not defined"
     ]
    }
   ],
   "source": [
    "# Lifted from http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# Plot the validation MSEs of all the models trained above with different optimizers\n",
    "for name in names:\n",
    "    plt.plot(histories[name].history['val_mean_squared_error'])\n",
    "plt.title('Validation MSEs with Different Optimizers')\n",
    "plt.ylabel('Validation MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0.0008, 0.0013)\n",
    "plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=2.0)\n",
    "plt.legend(names, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-12a55750f63c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Select the final model by picking the one with the lowest validation MSE (Adamax)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"CNN_2472350_Adamax\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_models/weights_best_val_MSE_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".hdf5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Select the final model by picking the one with the lowest validation MSE (Adamax)\n",
    "final_model = \"CNN_2472350_Adamax\"\n",
    "model.load_weights(\"saved_models/weights_best_val_MSE_\" + final_model + \".hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "\n",
    "## Step 7: Visualize the Loss and Test Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'histories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-25da363ad543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## TODO: Visualize the training and validation loss of your neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Lifted from http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_squared_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Train and Validation MSEs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'histories' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: Visualize the training and validation loss of your neural network\n",
    "# Lifted from http://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "plt.plot(histories[final_model].history['mean_squared_error'])\n",
    "plt.plot(histories[final_model].history['val_mean_squared_error'])\n",
    "plt.title('Model Train and Validation MSEs')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplots_adjust(left=0.0, right=2.0, bottom=0.0, top=2.0)\n",
    "plt.legend(['train', 'validation'], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Subset of the Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-c5d41201c91f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(X_test)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(X_test[i], y_test[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Completing the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Keypoints Detector\n",
    "\n",
    "Use the OpenCV face detection functionality you built in previous Sections to expand the functionality of your keypoints detector to color images with arbitrary size. Steps:\n",
    "1. Accept a color image.\n",
    "2. Convert the image to grayscale.\n",
    "3. Detect and crop the face contained in the image.\n",
    "4. Locate the facial keypoints in the cropped image.\n",
    "5. Overlay the facial keypoints in the original (color, uncropped) image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f7a0878a619f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in color image for face detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/sachinim.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert the image to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in color image for face detection\n",
    "image = cv2.imread('images/sachinim.jpg')\n",
    "\n",
    "# Convert the image to RGB colorspace\n",
    "image_copy = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot our image\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('image copy')\n",
    "ax1.imshow(image_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-705de5e6e8e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Convert the RGB  image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Extract the pre-trained face detector from an xml file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "### TODO: Use the face detection code we saw in Section 1 with your trained conv-net \n",
    "\n",
    "# Convert the RGB  image to grayscale\n",
    "gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Detect the faces in image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "\n",
    "# Make a copy of the orginal image to draw face detections on\n",
    "image_with_detections = np.copy(image_copy)\n",
    "\n",
    "# Get the bounding box for each detected face\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Add a red bounding box to the detections image\n",
    "    cv2.rectangle(image_with_detections, (x_face, y_face), (x_face+w_face, y_face+h_face), (255,0,0), 3)\n",
    "\n",
    "# Display the image with the detections\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax1.set_title('Image with Face Detection')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7690e277400c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# The following ndarray will hold our CNN input batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfaces_96x96_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m96\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Process each face individually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faces' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO : Paint the predicted keypoints on the test image\n",
    "\n",
    "# The following ndarray will hold our CNN input batch\n",
    "faces_96x96_cnn = np.ndarray(shape=(len(faces),96,96,1), dtype=float, order='F')\n",
    "\n",
    "# Process each face individually\n",
    "face_idx = 0\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Extract a face from the GREYSCALE image\n",
    "    face = gray[y_face:y_face+h_face,x_face:x_face+w_face]\n",
    "    # Resize it to match our traning set image size (96x96) and normalize it to be in [0,1]\n",
    "    face_96x96 = cv2.resize(face, (96,96), interpolation = cv2.INTER_CUBIC) / 255\n",
    "    # Add it to our CNN input batch\n",
    "    faces_96x96_cnn[face_idx,:,:,0] = face_96x96\n",
    "    face_idx = face_idx + 1\n",
    "\n",
    "# Validate CNN input batch shape\n",
    "print(faces_96x96_cnn.shape)\n",
    "\n",
    "# Predict facial keypoints\n",
    "facial_keypoints = model.predict(faces_96x96_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d1375cd6ace1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces_96x96_cnn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacial_keypoints\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faces' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1edfa6d1940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visually validate predictions\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(len(faces)):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(faces_96x96_cnn[i], facial_keypoints[i], ax)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5bcfe97edd1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Process each face individually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_face\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_face\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_face\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Extract a face from the GREYSCALE image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_face\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh_face\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_face\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_face\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw_face\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faces' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1edfa7126d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO : Paint the predicted keypoints on the test image\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# The following ndarray will hold our CNN 1-sample input batch\n",
    "faces_96x96_cnn = np.ndarray(shape=(1,96,96,1), dtype=float, order='F')\n",
    "\n",
    "# Process each face individually\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Extract a face from the GREYSCALE image\n",
    "    face = gray[y_face:y_face+h_face, x_face:x_face+w_face]\n",
    "    # Resize it to match our traning set image size (96x96) and normalize it to be in [0,1]\n",
    "    face_96x96 = cv2.resize(face, (96,96), interpolation = cv2.INTER_CUBIC) / 255\n",
    "    # Copy it in our CNN 1-sample input batch\n",
    "    faces_96x96_cnn[0,:,:,0] = face_96x96\n",
    "    # Predict facial keypoints\n",
    "    facial_keypoints = model.predict(faces_96x96_cnn)[0]\n",
    "    # Overlay the facial keypoints in the original (color, uncropped) image\n",
    "    # Refactored code from utils.py:\n",
    "    ax1.scatter(facial_keypoints[0::2] * (w_face / 2) + (w_face / 2) + x_face,\n",
    "                facial_keypoints[1::2] * (h_face / 2) + (h_face / 2) + y_face,\n",
    "                marker='o', c='c', s=40)\n",
    "\n",
    "# Display the image\n",
    "ax1.set_title('Image with Facial Keypoints')\n",
    "ax1.imshow(image_with_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a filter using facial keypoints to your laptop camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9a1039d2adf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time \n",
    "from keras.models import load_model\n",
    "\n",
    "def laptop_camera_go():\n",
    "    # Create instance of video capturer\n",
    "    cv2.namedWindow(\"Facial Keypoints [Press ESC to exit]\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    # Try to get the first frame\n",
    "    if vc.isOpened(): \n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    \n",
    "    # Extract the pre-trained face detector from an xml file\n",
    "    face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # The following ndarray will hold our CNN 1-sample input batch\n",
    "    faces_96x96_cnn = np.ndarray(shape=(1,96,96,1), dtype=float, order='F')\n",
    "    \n",
    "    # Keep video stream open\n",
    "    while rval:\n",
    "        # Convert the RGB video frame to grayscale and detect the face(s) in it\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "        \n",
    "        # Process each detected face to display bounding rectangle and facial keypoints\n",
    "        for (x_face,y_face,w_face,h_face) in faces:\n",
    "            # Extract a face from the GREYSCALE image\n",
    "            face = gray[y_face:y_face+h_face, x_face:x_face+w_face]\n",
    "            # Resize it to match our traning set image size (96x96) and normalize it to be in [0,1]\n",
    "            face_96x96 = cv2.resize(face, (96,96), interpolation = cv2.INTER_CUBIC) / 255\n",
    "            # Copy it in our CNN 1-sample input batch\n",
    "            faces_96x96_cnn[0,:,:,0] = face_96x96\n",
    "            # Predict facial keypoints\n",
    "            facial_keypoints = model.predict(faces_96x96_cnn)[0]\n",
    "            # Display each face rectangle in the color image\n",
    "            cv2.rectangle(frame, (x_face,y_face), (x_face+w_face,y_face+h_face), (0,0,255), 3)  \n",
    "            # Overlay the facial keypoints in the original (color, uncropped) image\n",
    "            facial_keypoints[0::2] = facial_keypoints[0::2] * (w_face / 2) + (w_face / 2) + x_face\n",
    "            facial_keypoints[1::2] = facial_keypoints[1::2] * (h_face / 2) + (h_face / 2) + y_face\n",
    "            for (x_point,y_point) in zip(facial_keypoints[0::2], facial_keypoints[1::2]):\n",
    "                cv2.circle(frame, (x_point, y_point), 3, (0,255,0), -1)\n",
    "            \n",
    "        # Plot image from camera with identity protected\n",
    "        cv2.imshow(\"Facial Keypoints [Press ESC to exit]\", frame)\n",
    "        \n",
    "        # Exit functionality\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # Exit by pressing ESC key\n",
    "            # Destroy windows\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            for i in range (1,5):\n",
    "                cv2.waitKey(1)\n",
    "            return\n",
    "        \n",
    "        # Read next frame\n",
    "        time.sleep(0.05)             # control framerate for computation - default 20 frames per sec\n",
    "        rval, frame = vc.read()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laptop_camera_go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-503658b0992c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run your keypoint face painter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'laptop_camera_go' is not defined"
     ]
    }
   ],
   "source": [
    "# Run your keypoint face painter\n",
    "laptop_camera_go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Adding a filter using facial keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d79e93ce8dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# cv2.IMREAD_UNCHANGED, this option is used because the sunglasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# image has a 4th channel that allows us to control how transparent each pixel in the image is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msunglasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images/sunglasses_4.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_UNCHANGED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Plot the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# Load in sunglasses image - note the usage of the special option\n",
    "# cv2.IMREAD_UNCHANGED, this option is used because the sunglasses \n",
    "# image has a 4th channel that allows us to control how transparent each pixel in the image is\n",
    "sunglasses = cv2.imread(\"images/sunglasses_4.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Plot the image\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.imshow(sunglasses)\n",
    "# Below we show the (guessed) bounding rectangle for the eyes underneath the sunglasses\n",
    "# We'll use the bounding rectangle to figure out how to map the glasses on the eye keypoints\n",
    "# Values were obtained via experimentation\n",
    "glasses_triangle_vertices = np.array([(280,220), (2800,220), (280,600)]).astype(np.float32)\n",
    "ax1.add_patch(\n",
    "    patches.Rectangle(\n",
    "        (280, 220),   # (x,y)\n",
    "        2800-280,          # width\n",
    "        600-220,          # height\n",
    "        alpha=0.6      # remove background\n",
    "    )\n",
    ")\n",
    "ax1.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sunglasses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-29104eabf429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print out the shape of the sunglasses image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'The sunglasses image has shape: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msunglasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sunglasses' is not defined"
     ]
    }
   ],
   "source": [
    "# Print out the shape of the sunglasses image\n",
    "print ('The sunglasses image has shape: ' + str(np.shape(sunglasses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sunglasses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3a272c94bb72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print out the sunglasses transparency (alpha) channel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0malpha_channel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msunglasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'the alpha channel here looks like'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha_channel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sunglasses' is not defined"
     ]
    }
   ],
   "source": [
    "# Print out the sunglasses transparency (alpha) channel\n",
    "alpha_channel = sunglasses[:,:,3]\n",
    "print ('the alpha channel here looks like')\n",
    "print (alpha_channel)\n",
    "\n",
    "# Just to double check that there are indeed non-zero values\n",
    "# Let's find and print out every value greater than zero\n",
    "values = np.where(alpha_channel != 0)\n",
    "print ('\\n the non-zero values of the alpha channel look like')\n",
    "print (values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-fac1cf29a0f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load in color image for face detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/obamas4.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert the image to RGB colorspace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in color image for face detection\n",
    "image = cv2.imread('images/obamas4.jpg')\n",
    "\n",
    "# Convert the image to RGB colorspace\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot the image\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('Original Image')\n",
    "ax1.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-15e55e8e483c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Extract the pre-trained face detector from an xml file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'detector_architectures/haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# The following ndarray will hold our CNN 1-sample input batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1edfd7cb048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## (Optional) TODO: Use the face detection code we saw in Section 1 with your trained conv-net to put\n",
    "## sunglasses on the individuals in our test image\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "# Extract the pre-trained face detector from an xml file\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# The following ndarray will hold our CNN 1-sample input batch\n",
    "faces_96x96_cnn = np.ndarray(shape=(1,96,96,1), dtype=float, order='F')\n",
    "\n",
    "# Convert the RGB video frame to grayscale and detect the face(s) in it\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "\n",
    "# Process each detected face to display bounding rec28tangle and facial keypoints\n",
    "for (x_face,y_face,w_face,h_face) in faces:\n",
    "    # Extract a face from the GREYSCALE image\n",
    "    face = gray[y_face:y_face+h_face, x_face:x_face+w_face]\n",
    "    # Resize it to match our traning set image size (96x96) and normalize it to be in [0,1]\n",
    "    face_96x96 = cv2.resize(face, (96,96), interpolation = cv2.INTER_CUBIC) / 255\n",
    "    # Copy it in our CNN 1-sample input batch\n",
    "    faces_96x96_cnn[0,:,:,0] = face_96x96\n",
    "    # Predict facial keypoints\n",
    "    facial_keypoints = model.predict(faces_96x96_cnn)[0]\n",
    "    # Extract the facial keypoints for the eyes only and remap them to the original image\n",
    "    facial_keypoints[0::2] = facial_keypoints[0::2] * (w_face / 2) + (w_face / 2) + x_face\n",
    "    facial_keypoints[1::2] = facial_keypoints[1::2] * (h_face / 2) + (h_face / 2) + y_face\n",
    "    eye_keypoints=[]\n",
    "    for (x_point,y_point) in zip(facial_keypoints[0:20:2], facial_keypoints[1:20:2]):\n",
    "        eye_keypoints.append((x_point, y_point))\n",
    "    # Compute the bounding rectangle for the eyes\n",
    "    eye_boundingRect = cv2.boundingRect(np.array(eye_keypoints).astype(np.float32))\n",
    "    # Build the triangle vertices needed by cv2.getAffineTransform()\n",
    "    eyes_triangle_vertices = np.array([(eye_boundingRect[0],eye_boundingRect[1]), (eye_boundingRect[0]+eye_boundingRect[2],eye_boundingRect[1]), (eye_boundingRect[0],eye_boundingRect[1]+eye_boundingRect[3])]).astype(np.float32)\n",
    "    # Compute the affine transform matrix from the two sets of three points (glasses and eyes)\n",
    "    map_matrix = cv2.getAffineTransform(glasses_triangle_vertices, eyes_triangle_vertices)\n",
    "    # Apply the affine transformation to the glasses\n",
    "    transformed_sunglasses = cv2.warpAffine(sunglasses, map_matrix, (image.shape[1], image.shape[0]))\n",
    "    # Build a binary mask of the pixels where the sunglasses are\n",
    "    transformed_sunglasses_mask = transformed_sunglasses[:,:,3] > 0\n",
    "    # Overwrite pixels in the original image with sunglasses pixels using their mask\n",
    "    image[:,:,:][transformed_sunglasses_mask] = transformed_sunglasses[:,:,0:3][transformed_sunglasses_mask]\n",
    "\n",
    "# Plot image\n",
    "ax1.set_title('Image with Sunglasses Painted On')\n",
    "ax1.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Adding a filter using facial keypoints to your laptop camera \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ae060f5a3769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time \n",
    "from keras.models import load_model\n",
    "\n",
    "def laptop_camera_go():\n",
    "    # Create instance of video capturer\n",
    "    cv2.namedWindow(\"Sunglasses Painter [Press ESC to exit]\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    # Try to get the first frame\n",
    "    if vc.isOpened(): \n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    \n",
    "    # Extract the pre-trained face detector from an xml file\n",
    "    face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # The following ndarray will hold our CNN 1-sample input batch\n",
    "    faces_96x96_cnn = np.ndarray(shape=(1,96,96,1), dtype=float, order='F')\n",
    "    \n",
    "    # Keep video stream open\n",
    "    while rval:\n",
    "        # Convert the RGB video frame to grayscale and detect the face(s) in it\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
    "        \n",
    "        # Process each detected face to display bounding rectangle and facial keypoints\n",
    "        for (x_face,y_face,w_face,h_face) in faces:\n",
    "            # Extract a face from the GREYSCALE image\n",
    "            face = gray[y_face:y_face+h_face, x_face:x_face+w_face]\n",
    "            # Resize it to match our traning set image size (96x96) and normalize it to be in [0,1]\n",
    "            face_96x96 = cv2.resize(face, (96,96), interpolation = cv2.INTER_CUBIC) / 255\n",
    "            # Copy it in our CNN 1-sample input batch\n",
    "            faces_96x96_cnn[0,:,:,0] = face_96x96\n",
    "            # Predict facial keypoints\n",
    "            facial_keypoints = model.predict(faces_96x96_cnn)[0]\n",
    "            # Extract the facial keypoints for the eyes only and remap them to the original image\n",
    "            facial_keypoints[0::2] = facial_keypoints[0::2] * (w_face / 2) + (w_face / 2) + x_face\n",
    "            facial_keypoints[1::2] = facial_keypoints[1::2] * (h_face / 2) + (h_face / 2) + y_face\n",
    "            eye_keypoints=[]\n",
    "            for (x_point,y_point) in zip(facial_keypoints[0:20:2], facial_keypoints[1:20:2]):\n",
    "                eye_keypoints.append((x_point, y_point))\n",
    "            # Compute the bounding rectangle for the eyes\n",
    "            eye_boundingRect = cv2.boundingRect(np.array(eye_keypoints).astype(np.float32))\n",
    "            # Build the triangle vertices needed by cv2.getAffineTransform()\n",
    "            eyes_triangle_vertices = np.array([(eye_boundingRect[0],eye_boundingRect[1]), (eye_boundingRect[0]+eye_boundingRect[2],eye_boundingRect[1]), (eye_boundingRect[0],eye_boundingRect[1]+eye_boundingRect[3])]).astype(np.float32)\n",
    "            # Compute the affine transform matrix from the two sets of three points (glasses and eyes)\n",
    "            map_matrix = cv2.getAffineTransform(glasses_triangle_vertices, eyes_triangle_vertices)\n",
    "            # Apply the affine transformation to the glasses\n",
    "            transformed_sunglasses = cv2.warpAffine(sunglasses, map_matrix, (frame.shape[1], frame.shape[0]))\n",
    "            # Build a binary mask of the pixels where the sunglasses are\n",
    "            transformed_sunglasses_mask = transformed_sunglasses[:,:,3] > 0\n",
    "            # Overwrite pixels in the original image with sunglasses pixels using their mask\n",
    "            frame[:,:,:][transformed_sunglasses_mask] = transformed_sunglasses[:,:,0:3][transformed_sunglasses_mask]\n",
    "            \n",
    "        # Plot image from camera with sunglasses painted on\n",
    "        cv2.imshow(\"Sunglasses Painter [Press ESC to exit]\", frame)\n",
    "        \n",
    "        # Exit functionality\n",
    "        key = cv2.waitKey(20)\n",
    "        if key == 27: # Exit by pressing ESC key\n",
    "            # Destroy windows\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            for i in range (1,5):\n",
    "                cv2.waitKey(1)\n",
    "            return\n",
    "        \n",
    "        # Read next frame\n",
    "        time.sleep(0.05)             # control framerate for computation - default 20 frames per sec\n",
    "        rval, frame = vc.read()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'laptop_camera_go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-8554c538ef77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run sunglasses painter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlaptop_camera_go\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'laptop_camera_go' is not defined"
     ]
    }
   ],
   "source": [
    "# Run sunglasses painter\n",
    "laptop_camera_go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen cap of the output of laptop_camera_go():\n",
    "\n",
    "![](images/Sunglasses Painter [Press ESC to exit]2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
